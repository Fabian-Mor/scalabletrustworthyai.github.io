+++
abstract = "Accurate uncertainty estimation is vital to trustworthy machine learning, yet uncertainties typically have to be learned for each task anew. This work introduces the first pretrained uncertainty modules for vision models. Similar to standard pretraining this enables the zero-shot transfer of uncertainties learned on a large pretraining dataset to specialized downstream datasets. We enable our large-scale pretraining on ImageNet-21k by solving a gradient conflict in previous uncertainty modules and accelerating the training by up to 180x. We find that the pretrained uncertainties generalize to unseen datasets. In scrutinizing the learned uncertainties, we find that they capture aleatoric uncertainty, disentangled from epistemic components. We demonstrate that this enables safe retrieval and uncertainty-aware dataset visualization. To encourage applications to further problems and domains, we release all pretrained checkpoints and code."
date = "2024-02-26T00:00:00+00:00"
image = "kirchhof2024pretrained.png"
math = false
publication = "arXiv"
publication_short = "arXiv"
selected = false
title = "Pretrained Visual Uncertainties"
url_code = "//github.com/mkirchhof/url"
url_dataset = ""
url_pdf = "//arxiv.org/abs/2402.16569"
url_project = ""
url_video = ""

[[authors]]
    name = "Michael Kirchhof"
    is_member = true
    id = "michael"
[[authors]]
    name = "Mark Collier"
    is_member = false
[[authors]]
    name = "Seong Joon Oh"
    is_member = true
    id = "joon"
[[authors]]
    name = "Enkelejda Kasneci"
    is_member = false
+++
